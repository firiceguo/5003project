{"nbformat_minor": 2, "cells": [{"execution_count": 17, "cell_type": "code", "source": "from pyspark.ml.linalg import Vectors, SparseVector, DenseVector\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import GBTRegressor\nfrom collections import defaultdict\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nimport os\nimport json\nimport cPickle as pickle\nimport math\nfrom pyspark.sql import functions as F\n\n\ndef tuple2sparse(tp, size=43, begin=19, end=42):\n    dic = {}\n    for i in xrange(end-begin):\n        if (tp[i] - 0) > 10e-4:\n            dic[i+begin] = tp[i]\n    v = Vectors.sparse(size, dic)\n    return v\n\n\ndef add(v1, v2):\n    assert isinstance(v1, SparseVector) and isinstance(v2, SparseVector), 'One of them is not SparseVector!'\n    assert v1.size == v2.size, 'Size not equal!'\n    values = defaultdict(float) # Dictionary with default value 0.0\n    # Add values from v1\n    for i in range(v1.indices.size):\n        values[v1.indices[i]] += v1.values[i]\n    # Add values from v2\n    for i in range(v2.indices.size):\n        values[v2.indices[i]] += v2.values[i]\n    return Vectors.sparse(v1.size, dict(values))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 31, "cell_type": "code", "source": "def loadDataJson(business_path='', user_path='', star_path=''):\n    bDF = spark.read.json(business_path)\n    uDF = spark.read.json(user_path)\n    sDF = spark.read.json(star_path)\n\n    businessDF = bDF.rdd.map(lambda x: (x['b_id'], tuple2sparse(\n                         tuple(x['loc']) + tuple(x['votes']) + (x['avg_star'], ) +\n                         tuple(x['cates']) + (x['rev_num'], ) + tuple(x['ckins']),\n                         begin=19, end=42))).toDF(['b_id', 'b_features'])\n\n    userDF = uDF.rdd.map(lambda x: (x['u_id'], tuple2sparse(\n                     tuple(x['loc']) + tuple(x['votes']) +\n                     (x['loc_num'], x['avg_star'], x['rev_num']) + tuple(x['cates']),\n                     begin=0, end=19))).toDF(['u_id', 'u_features'])\n\n    starDF = sDF.select((sDF.business_id).alias('b_id'), (sDF.user_id).alias('u_id'), \n                        (sDF.stars).alias('label'), (sDF.review_id).alias('rev_id'))\n    return businessDF, userDF, starDF\n\n\ndef transData4GBT(businessDF, userDF, starDF):\n    alldata = starDF.select(starDF.b_id, starDF.u_id, starDF.label) \\\n                    .join(businessDF, starDF.b_id == businessDF.b_id).drop(businessDF.b_id) \\\n                    .join(userDF, starDF.u_id == userDF.u_id).drop(userDF.u_id)\\\n                    .select('label', 'b_features', 'u_features', 'u_id', 'b_id')\n    assembler = VectorAssembler(\n                    inputCols=[\"b_features\", \"u_features\"],\n                    outputCol=\"features\")\n\n    data = assembler.transform(alldata).drop('b_features', 'u_features')\n    return data\n\n\ndef traingbt(datafrom='json', business_path='', user_path='', star_path=''):\n    gbt = GBTRegressor(maxIter=5, maxDepth=2, seed=42)\n    if datafrom == 'json':\n        businessDF, userDF, starDF = loadDataJson(business_path=business_path,\n                                                  user_path=user_path,\n                                                  star_path=star_path)\n    elif datafrom == 'mongodb':\n        businessDF, userDF, starDF = loadDataMongo()\n    data = transData4GBT(businessDF, userDF, starDF)\n    model = gbt.fit(data)\n    return model", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 19, "cell_type": "code", "source": "def recommendation(businessDF, userDF, testDF, model):\n    CartesianDF = testDF.crossJoin(businessDF.select('b_id')).drop(testStarDF.b_id).drop('rev_id')\n    recDF = transData4GBT(businessDF, userDF, CartesianDF)\n    predDF = model.transform(recDF)\n    \n    temp = predDF.groupby('u_id').agg(F.max(predDF.prediction)) \\\n                 .withColumnRenamed('max(prediction)', 'prediction')\n    pred = temp.join(predDF, ['prediction', 'u_id'], 'outer').drop(predDF.u_id).drop(predDF.prediction)\n    pred = pred.select('u_id', 'b_id')\n\n    return pred", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 34, "cell_type": "code", "source": "business_path = 'businesses.json'\nuser_path = 'users.json'\nstar_path = 'yelp_academic_dataset_review.json'\n\n\ngbt = GBTRegressor(maxIter=50, maxDepth=6, seed=42)\nbusinessDF, userDF, starDF = loadDataJson(business_path=business_path,\n                                          user_path=user_path,\n                                          star_path=star_path)\n# split starDF to training data and test data\ntrainStarDF, testStarDF = starDF.randomSplit([0.7, 0.3])\n\ntrainDF = transData4GBT(businessDF, userDF, trainStarDF)\n\nmodel = gbt.fit(trainDF)\n\ntestDF = transData4GBT(businessDF, userDF, testStarDF)\npredDF = model.transform(testDF)\n\npredDF.show()\nerrors = predDF.rdd.map(lambda x: (x.label - x.prediction)**2).collect()\nRMSE = math.sqrt(sum(errors)/len(errors))\nprint 'RMSE: %.8f' % RMSE\n\n# recDF = recommendation(businessDF, testStarDF, model)\n# recDF.printSchema()\n# recDF.show()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------------------+--------------------+--------------------+------------------+\n|label|                u_id|                b_id|            features|        prediction|\n+-----+--------------------+--------------------+--------------------+------------------+\n|    5|-1zQA2f_syMAdA04P...|aNe8ofTYrealxqv7V...|(86,[19,21,22,23,...| 2.624166121228215|\n|    2|-3i9bhfvrM3F1wsC9...|5iSmZO0SrKU6EoXK_...|(86,[19,21,22,23,...|  3.63287926235794|\n|    4|-3i9bhfvrM3F1wsC9...|ghpFh6XpH1TYZhjAG...|(86,[19,21,22,23,...| 4.274533711932253|\n|    4|-3i9bhfvrM3F1wsC9...|GtHu9uGXpn7Jg_Z7v...|(86,[19,21,22,23,...|3.8912370468409585|\n|    4|-4Anvj46CWf57KWI9...|vKA9sIqBcW0UlTKGh...|(86,[19,21,22,23,...|3.3628979446389167|\n|    4|-55DgUo52I3zW9Rxk...|3qlqzQrwh8hjBltlg...|(86,[19,21,22,23,...|  4.57561898326436|\n|    5|-55DgUo52I3zW9Rxk...|CYWRPE-1IHPBb-zfF...|(86,[19,21,22,23,...|4.5735588804823175|\n|    5|-55DgUo52I3zW9Rxk...|3awTUGMdUVrwEBkFF...|(86,[19,21,22,23,...| 4.580856617019137|\n|    4|-55DgUo52I3zW9Rxk...|6t98--hqg8suYkm_3...|(86,[19,21,22,23,...| 4.322059431814168|\n|    5|-7JSlmBJKUQwREG_y...|QD0gnPAdy7w2vZZG9...|(86,[19,21,22,23,...| 4.602464715419953|\n|    1|-7V6r0PLuBlFVjbLJ...|x8O-Mll5ksDpeIgtA...|(86,[19,21,22,23,...|1.6449891781854125|\n|    4|-9da1xk7zgnnfO1uT...|PXShA3JZMXr2mEH3o...|(86,[19,21,22,23,...| 4.517274403933761|\n|    4|-9da1xk7zgnnfO1uT...|_YUcCnJXjUgkS9fSn...|(86,[19,21,22,23,...| 4.259394274403336|\n|    5|-9da1xk7zgnnfO1uT...|7dHYudt6OOIjiaxkS...|(86,[19,21,22,23,...|  4.17251951814607|\n|    4|-9da1xk7zgnnfO1uT...|EUWBT5GDxPC95w9it...|(86,[19,21,22,23,...| 3.863477352529793|\n|    2|-9da1xk7zgnnfO1uT...|lYCeqldIiOggsbByH...|(86,[19,21,22,23,...|2.6200234809776504|\n|    3|-9da1xk7zgnnfO1uT...|q18xbq3Cbyp_BJyfM...|(86,[19,21,22,23,...|  3.96319528777114|\n|    5|-9da1xk7zgnnfO1uT...|y0x795PyDX8JL_oyI...|(86,[19,21,22,23,...| 3.863477352529793|\n|    5|-9da1xk7zgnnfO1uT...|yofHPSC24EsWTMJq3...|(86,[19,21,22,23,...|3.6645203093244483|\n|    4|-9da1xk7zgnnfO1uT...|jaJnPIX9VxsFyfV5z...|(86,[19,21,22,23,...| 4.266858412775395|\n+-----+--------------------+--------------------+--------------------+------------------+\nonly showing top 20 rows\n\nRMSE: 1.05118689"}], "metadata": {"collapsed": false}}, {"execution_count": 32, "cell_type": "code", "source": "# recDF = recommendation(businessDF, userDF, testStarDF, model)\n# recDF.printSchema()\n# recDF.show()\n\nCartesianDF = testStarDF.crossJoin(businessDF.select('b_id')).drop(testStarDF.b_id).drop('rev_id')\nCartesianDF.printSchema()\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- u_id: string (nullable = true)\n |-- label: long (nullable = true)\n |-- b_id: string (nullable = true)"}], "metadata": {"collapsed": false}}, {"execution_count": 33, "cell_type": "code", "source": "recDF = transData4GBT(businessDF, userDF, CartesianDF)\n# predDF = model.transform(recDF)\n    \n# temp = predDF.groupby('u_id').agg(F.max(predDF.prediction)) \\\n#                  .withColumnRenamed('max(prediction)', 'prediction')\n# pred = temp.join(predDF, ['prediction', 'u_id'], 'outer').drop(predDF.u_id).drop(predDF.prediction)\n# pred = pred.select('u_id', 'b_id')\n\nrecDF.printSchema()", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error occurred while calling o1090.transform.\n: org.apache.spark.SparkException: Job 46 cancelled because Stage 214 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1364)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1363)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1363)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1363)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2378)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2780)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2377)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2120)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2119)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2810)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2119)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2128)\n\tat org.apache.spark.sql.Dataset.first(Dataset.scala:2135)\n\tat org.apache.spark.ml.feature.VectorAssembler.first$lzycompute$1(VectorAssembler.scala:57)\n\tat org.apache.spark.ml.feature.VectorAssembler.org$apache$spark$ml$feature$VectorAssembler$$first$1(VectorAssembler.scala:57)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2$$anonfun$1.apply$mcI$sp(VectorAssembler.scala:88)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2$$anonfun$1.apply(VectorAssembler.scala:88)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2$$anonfun$1.apply(VectorAssembler.scala:88)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2.apply(VectorAssembler.scala:88)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2.apply(VectorAssembler.scala:58)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:58)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 30, in transData4GBT\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\", line 105, in transform\n    return self._transform(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 252, in _transform\n    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\nPy4JJavaError: An error occurred while calling o1090.transform.\n: org.apache.spark.SparkException: Job 46 cancelled because Stage 214 was cancelled\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply$mcVI$sp(DAGScheduler.scala:1364)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1363)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleStageCancellation$1.apply(DAGScheduler.scala:1363)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:234)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1363)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2378)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2780)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2377)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2120)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2119)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2810)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2119)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2128)\n\tat org.apache.spark.sql.Dataset.first(Dataset.scala:2135)\n\tat org.apache.spark.ml.feature.VectorAssembler.first$lzycompute$1(VectorAssembler.scala:57)\n\tat org.apache.spark.ml.feature.VectorAssembler.org$apache$spark$ml$feature$VectorAssembler$$first$1(VectorAssembler.scala:57)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2$$anonfun$1.apply$mcI$sp(VectorAssembler.scala:88)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2$$anonfun$1.apply(VectorAssembler.scala:88)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2$$anonfun$1.apply(VectorAssembler.scala:88)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2.apply(VectorAssembler.scala:88)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$2.apply(VectorAssembler.scala:58)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:58)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n"}], "metadata": {"collapsed": false}}, {"execution_count": 30, "cell_type": "code", "source": "predDF = model.transform(recDF)\npredDF.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- label: long (nullable = true)\n |-- b_features: vector (nullable = true)\n |-- u_features: vector (nullable = true)\n |-- u_id: string (nullable = true)\n |-- b_id: string (nullable = true)\n |-- features: vector (nullable = true)\n |-- prediction: double (nullable = true)"}], "metadata": {"collapsed": false}}, {"execution_count": 39, "cell_type": "code", "source": "from pyspark.sql import functions as F\ntemp = prediction.groupby('u_id').agg(F.max(prediction.prediction)) \\\n                 .withColumnRenamed('max(prediction)', 'prediction')\npred = temp.join(prediction, ['prediction', 'u_id'], 'outer').drop(prediction.u_id).drop(prediction.prediction)\n\npred.printSchema()\npred.show()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- prediction: double (nullable = true)\n |-- u_id: string (nullable = true)\n |-- label: double (nullable = true)\n |-- b_id: string (nullable = true)\n |-- features: vector (nullable = true)\n\n+------------------+--------------------+-----+--------------------+--------------------+\n|        prediction|                u_id|label|                b_id|            features|\n+------------------+--------------------+-----+--------------------+--------------------+\n|1.5334101408098666|09D5rRgsFmHRJKVBm...|  2.0|EtLmuDIMsBCmqdgpy...|(43,[0,5,6,7,8,11...|\n|1.5334101408098666|1-VO40rPQDC4Q9Spt...|  1.0|K0pN6x7fzmsO8h36N...|(43,[0,5,6,7,9,10...|\n|1.5334101408098666|2ezaoRp1PzHaMgIrz...|  1.0|sPd3E7lFzd_yooiq-...|(43,[0,5,6,7,8,18...|\n|1.5334101408098666|4-ElUwzF5CgbEy0ay...|  1.0|FR7dh1_TnWNyGbhhq...|(43,[0,5,6,7,12,1...|\n|1.5334101408098666|4c-dhmNntBrpUHOCc...|  1.0|vGLl5xum2u2Qf8_Av...|(43,[0,5,6,7,8,9,...|\n|1.5334101408098666|4c-dhmNntBrpUHOCc...|  1.0|MTtI2bqoNHN_0m2cH...|(43,[0,5,6,7,8,9,...|\n|1.5334101408098666|5Acw0JaxH1A_hDC3S...|  1.0|LIPtg0tDFCNTt-fqb...|(43,[0,1,5,6,7,8,...|\n|1.5334101408098666|5qKemxZi2wnA0NzKr...|  1.0|_HVZ1V8IDa49MWdej...|(43,[0,5,6,7,8,18...|\n|1.5334101408098666|6Uc4bSDRcrwA-kx1W...|  1.0|WfroD4iB5M1nFw8j8...|(43,[0,5,6,7,12,1...|\n|1.5334101408098666|6qwKduiMppfvjJNZB...|  1.0|XG8dARktPWMFpBiQG...|(43,[0,5,6,7,8,10...|\n|1.5334101408098666|74UIfuojXgxw3df1C...|  1.0|VnS1QKpsfGj_7c058...|(43,[0,5,6,7,12,1...|\n|1.5334101408098666|85pNTdsC6DWqfifX4...|  1.0|2uLU7C6-59QKdiTaw...|(43,[0,2,5,6,7,19...|\n|1.5334101408098666|89Fmu93aliAeF2-6U...|  1.0|_V0yJdpXrbdKzBDoV...|(43,[0,5,6,7,8,10...|\n|1.5334101408098666|AVJYfsEnp-pXFynk5...|  1.0|YfDmf2hBB8jEdKIEX...|(43,[0,2,5,6,7,9,...|\n|1.5334101408098666|AxqbtVrhqubobl5OM...|  1.0|II-vMV6s9Ke6l9V7j...|(43,[0,5,6,7,9,11...|\n|1.5334101408098666|C7F_PCbwjx3yIaqXp...|  1.0|vvvDtPXzZHnAYxECh...|(43,[0,5,6,7,16,1...|\n|1.5334101408098666|CRYbqNcA31OautCr2...|  1.0|_THIu8AX6CyBmP_3p...|(43,[0,3,5,6,7,8,...|\n|1.5334101408098666|ColBn9YdAVZ0HYSpH...|  2.0|HuzhmzDHcI66G1744...|(43,[0,2,3,4,5,6,...|\n|1.5334101408098666|F1CP23wUsStv5ObB-...|  1.0|Zo0DWTyHTSyKRVIWI...|(43,[0,5,6,7,8,10...|\n|1.5334101408098666|HpxYfwGSLI2uiGZ1q...|  1.0|fkfVkLnoPNgVddCy0...|(43,[0,5,6,7,8,10...|\n+------------------+--------------------+-----+--------------------+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 34, "cell_type": "code", "source": "businessDF.printSchema()\nuserDF.printSchema()\nstarDF.printSchema()\ntrainStarDF.printSchema()\nprediction.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- b_id: string (nullable = true)\n |-- b_features: vector (nullable = true)\n\nroot\n |-- u_id: string (nullable = true)\n |-- u_features: vector (nullable = true)\n\nroot\n |-- b_id: string (nullable = true)\n |-- u_id: string (nullable = true)\n |-- label: long (nullable = true)\n |-- rev_id: string (nullable = true)\n\nroot\n |-- b_id: string (nullable = true)\n |-- u_id: string (nullable = true)\n |-- label: long (nullable = true)\n |-- rev_id: string (nullable = true)\n\nroot\n |-- label: double (nullable = true)\n |-- u_id: string (nullable = true)\n |-- b_id: string (nullable = true)\n |-- features: vector (nullable = true)\n |-- prediction: double (nullable = true)"}], "metadata": {"collapsed": false}}, {"execution_count": 50, "cell_type": "code", "source": "CartesianDF = testStarDF.crossJoin(businessDF.select('b_id')).drop(testStarDF.b_id).drop('rev_id')\nCartesianDF.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+-----+--------------------+\n|                u_id|label|                b_id|\n+--------------------+-----+--------------------+\n|dVy5EAV9YZIl9Xl-X...|    3|QgNYM-ccNhJ8eGsQP...|\n|dVy5EAV9YZIl9Xl-X...|    3|BqsIt1BQKzS-hEKLY...|\n|dVy5EAV9YZIl9Xl-X...|    3|MH0oOCJ7DKnIJWwUQ...|\n|dVy5EAV9YZIl9Xl-X...|    3|s9ZY6ESOJF0mABkGr...|\n|dVy5EAV9YZIl9Xl-X...|    3|ln8nvcRttTQTZeDjc...|\n|dVy5EAV9YZIl9Xl-X...|    3|f8e1MH4YvIY1Km7W2...|\n|dVy5EAV9YZIl9Xl-X...|    3|llifBVCFAnr124WdK...|\n|dVy5EAV9YZIl9Xl-X...|    3|FiB1rfmgaED4mmHpO...|\n|dVy5EAV9YZIl9Xl-X...|    3|lZaBsXK-vhxL1Ck8E...|\n|dVy5EAV9YZIl9Xl-X...|    3|krBpN5vbCQrB54QvT...|\n|dVy5EAV9YZIl9Xl-X...|    3|xxjxUM-VK4N33LN8N...|\n|dVy5EAV9YZIl9Xl-X...|    3|qkjOhzGvUPSdKX7ss...|\n|dVy5EAV9YZIl9Xl-X...|    3|2G_6PBM-klbh1u2v2...|\n|dVy5EAV9YZIl9Xl-X...|    3|GD9mTnCht2bog2yb0...|\n|dVy5EAV9YZIl9Xl-X...|    3|E0T-xQJXpM6Hsm-Ee...|\n|dVy5EAV9YZIl9Xl-X...|    3|sPvjzXjzvGFwBLwtn...|\n|dVy5EAV9YZIl9Xl-X...|    3|8u6NUtxSPH3CbLKTQ...|\n|dVy5EAV9YZIl9Xl-X...|    3|qJRZ7eaarbermHS3Z...|\n|dVy5EAV9YZIl9Xl-X...|    3|rslX_CGOBr5m6yOor...|\n|dVy5EAV9YZIl9Xl-X...|    3|dQ11s7taakRn8omBU...|\n+--------------------+-----+--------------------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"execution_count": 17, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pysparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python2", "name": "pyspark", "codemirror_mode": {"version": 2, "name": "python"}}}}